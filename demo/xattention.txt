XAttention Title: Unlocking the Power of Block Sparse Attention with Antidiagonal Scoring
XAttention Abstract: Long-Context Transformer Models (LCTMs) arevital for real-world applications but suffer highcomputational costs due to attention’s quadraticcomplexity. Block-sparse attention mitigates thisby focusing computation on critical regions, yetexisting methods struggle with balancing accuracy and efficiency due to costly block importance measurements. In this paper, we introduce XAttention, a plug-and-play framework thatdramatically accelerates long-context inferencein Transformers models using sparse attention.XAttention’s key innovation is the insight thatthe sum of antidiagonal values (i.e., from thelower-left to upper-right) in the attention matrix provides a powerful proxy for block importance. This allows for precise identification andpruning of non-essential blocks, resulting in highsparsity and dramatically accelerated inference.Across comprehensive evaluations on demanding long-context benchmarks—including RULERand LongBench for language, VideoMME forvideo understanding, and VBench for video generation—XAttention achieves accuracy comparable to full attention while delivering substantialcomputational gains. We demonstrate up to 13.5×acceleration in attention computation. These results underscore XAttention’s ability to unlockthe practical potential of block sparse attention,paving the way for scalable and efficient deployment of LCTMs in real-world applications. Codewill be made publicly available upon publication.